{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is split up into 3 Sections\n",
    "- Data Retrieval\n",
    "- Data Assessment\n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse data, first of all data must be made available. Data was gathered using 3 methods.\n",
    "Firstly, data was made available by directly downloading a CSV file \"twitter-archive-enhanced.csv\"\n",
    "The file was made available via the nana degree resources folder on Udacity. This first csv file was used to create the first dataframe \"df_archive\"\n",
    "\n",
    "\n",
    "Following this, efforts progressed onto retrieving data by programattically downloading the data from the internet using the requests module. The data was read from the url \"[https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\" and the contents were stored locally in a file \"image-predictions.tsv\". This second tsv file was used to create the second dataframe \"df_images\"\n",
    "\n",
    "Lastly, additional data for analysis was gotten via the Twitter API. Using the tweepy module, an access library for the Twitter REST API, requests for tweet info were made directly to twitter and the output was stored locally in a flat file \"tweet_json,txt\". \n",
    "In order to access the Twitter API, it was necessary to acquire secret keys, consumer keys, bearer tokens and the likes. These keys were gotten from the Twitter Developer Portal after twitter approved my request for elevated access to their API. \n",
    "Also note that, the tweet_json.txt file had to be built iteratively as the twitter API only allows a certain maximum of 900 requests within a 15 minute interval.\n",
    "\n",
    "With these 3 files handy, 3 dataframes were created (\"df_archive\",\"df_images\", \"df_all_json\")\n",
    "\n",
    "[Click for Data Gathering code](wrangle_act.ipynb#Data-Gathering)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data assessment was done using a plethora of techniques from visual/manual assessment to programmatic assessment.\n",
    "\n",
    "The aim of the data assessment stage was to certify that the acquired data was of high quality, devoid of **quality** and **tidiness** issues\n",
    "Such quality and tidiness issues could pose challenges to accurate data analysis, and therefore could also lead to wrong inferences and conclusions. Therefore, in the data assessment stage, a list of quality and tidiness issues were identified and documented. The identified isses were then addressed in the data cleaning stage.\n",
    "\n",
    "Below are a list of issues identified during data assessment\n",
    "\n",
    "#### Quality issues\n",
    "\n",
    "1. tweet_archive_enhanced.csv includes replies and retweets which are not meant to feature in analysis (df_archive)\n",
    "\n",
    "2. timestamp column should be represented as a datetime instead of as a string (df_archive)\n",
    "\n",
    "3. rating_numerator and rating denominator column should be represended as a float as these columns could contain floating numbers (df_archive)\n",
    "\n",
    "4. 23 records in the the tweet_archive_enchanced.csv have inaccurate data as the denominators are less than 10  (df_archive)\n",
    "    re-compute inaccurate ratings using tweet text\n",
    "\n",
    "5. 543 records found in image_prediction.tsv which were not recognized as dogs using first prediction method  (df_images)\n",
    "\n",
    "6. tweet_id column in the tweet_archive_enchanced.csv should be represented as an object instead of as an integer as it is not treated as a numerical field. The same applies to tweet_id column in the image_prediction.tsv and  id in the tweet_json.txt (df_archive, df_images, df_all_json)\n",
    "\n",
    "7. 55 dogs are recorded as having the name \"**a**\" in the tweet_archive_enchanced.csv this is an inaccurate name and needs to be looked into. Some other inaccurate names were spotted also  (df_archive)\n",
    "\n",
    "8. 31 tweet records could not be retrieved using tweepy into tweet_json.txt, therefore the data on favorite count cannot be derived.  (df_all_json)\n",
    "\n",
    "#### Tidiness issues\n",
    "1. The dog classification of doggo, pupper, puppo, floffer should be collapsed into 1 column\n",
    "\n",
    "2. Since a dog has only one breed (1 to 1 relationship) the dog breed name should be extracted from the image_prediction.tsv and appended to the tweet_archive.csv. This also applies to the jpg_url field. Also the retweet_count and the favorites_count field should be extracted from the tweet_json.txt dataset and appended onto the tweet_archive_enhanced dataset as this is also a 1-to-1 relationship. \n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data cleaning stage, the quality / tidiness issues earlier identified were eliminated or better understood.\n",
    "Issues such as wrong ratings where addressed by extracting the correct ratings from the tweet text and replacing the rating value with the extracted.\n",
    "Some other issues such as the presence of null values as documented in issue 1, 2 and 3 were addressed by simply dropping those colums.\n",
    "As the analysis was focued on dog ratings, through data cleaning it became imperative to drop all records that had images which were not predicted as dog images. \n",
    "\n",
    "Methods ranging from substitution of values, dropping unused columns, dropping null values were used in the cleaning process\n",
    "\n",
    "[Click for Consolidated Cleaning Code](wrangle_act.ipynb#Consolidated-quality-cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Following the data retrieval, data assessment and data cleaning, more accurate insights and visualizations were able to be derived from the data sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
